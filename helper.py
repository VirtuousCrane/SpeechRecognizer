"""
Use this script to create JSON-Line description files that can be used to
train deep-speech models through this library.
This works with data directories that are organized like LibriSpeech:
data_directory/group/speaker/[file_id1.wav, file_id2.wav, ...,
                              speaker.trans.txt]

Where speaker.trans.txt has in each line, file_id transcription

NOTE: this file is from the https://github.com/baidu-research/ba-dls-deepspeech repository
"""

from __future__ import absolute_import, division, print_function

import json
import os
import librosa
import pickle
import numpy as np
import random
from pprint import pprint
from random import shuffle
from IPython.display import Audio
from keras import backend as K
from keras.layers import (Input, Lambda)
from keras.models import Model
from keras.optimizers import SGD
from keras.callbacks import ModelCheckpoint


# Transcript creating functions:
def create_transcript_from_folder(data_directory, group, speaker, output_file):
    labels = []
    durations = []
    keys = []
    dat_di = data_directory + '/' + group + '/' + speaker
    labels_file = os.path.join(dat_di,
                                '{}-{}.trans.txt'
                                .format(group, speaker))
    for line in open(labels_file):
        split = line.strip().split()
        file_id = split[0]
        label = ' '.join(split[1:]).lower()
        audio_file = os.path.join(data_directory, group, speaker,
                                  file_id) + '.flac'
        timeSeries, samplingRate = librosa.load(audio_file)
        duration = librosa.get_duration(timeSeries, samplingRate)
        keys.append(audio_file)
        durations.append(duration)
        labels.append(label)
    with open(output_file, 'w') as out_file:
        for i in range(len(keys)):
            line = json.dumps({'key': keys[i], 'duration': durations[i],
                              'text': labels[i]})
            out_file.write(line + '\n')
    print('Done creating {}'.format(output_file))


def create_transcript(data_directory, groups, output_file):
    labels = []
    durations = []
    keys = []
    for group in groups:
        speaker_path = os.path.join(data_directory, group)
        for speaker in os.listdir(speaker_path):
            transcript = os.path.join(speaker_path, speaker, '{}-{}.trans.txt'.format(group, speaker))
            for line in open(transcript):
                split = line.strip().split()
                file_id = split[0]
                label = ' '.join(split[1:]).lower()
                audio_file = os.path.join(data_directory, group, speaker,
                                          file_id) + '.flac'
                timeSeries, samplingRate = librosa.load(audio_file)
                duration = librosa.get_duration(timeSeries, samplingRate)
                keys.append(audio_file)
                durations.append(duration)
                labels.append(label)
            with open(output_file, 'w') as out_file:
                for i in range(len(keys)):
                    line = json.dumps({'key': keys[i], 'duration': durations[i],
                                      'text': labels[i]})
                    out_file.write(line + '\n')
    print('Done creating {}'.format(output_file))

    
def create(data_directory, output_file):
    labels = []
    durations = []
    keys = []
    for group in os.listdir(data_directory):
        speaker_path = os.path.join(data_directory, group)
        for speaker in os.listdir(speaker_path):
            labels_file = os.path.join(speaker_path, speaker,
                                       '{}-{}.trans.txt'
                                       .format(group, speaker))
            for line in open(labels_file):
                split = line.strip().split()
                file_id = split[0]
                label = ' '.join(split[1:]).lower()
                audio_file = os.path.join(speaker_path, speaker,
                                          file_id) + '.flac'
                timeSeries, samplingRate = librosa.load(audio_file)
                duration = librosa.get_duration(timeSeries, samplingRate)
                keys.append(audio_file)
                durations.append(duration)
                labels.append(label)
    with open(output_file, 'w') as out_file:
        for i in range(len(keys)):
            line = json.dumps({'key': keys[i], 'duration': durations[i],
                              'text': labels[i]})
            out_file.write(line + '\n')


#Transcript management:
def train_test_val_split(desc_file):
    """ 
    Reads data from a JSON-like file (generated by create_desc_json.py) and returns train_path, train_duration, train_text, test_path, test_duration, test_text, validation_path, validation_duration, validation_text
    """
    train_path, test_path, validation_path = [], [], []
    train_duration, test_duration, validation_duration = [], [], []
    train_text, test_text, validation_text = [], [], []
    
    with open(desc_file) as f:
        '''
        data = f.read().split('\n')
        shuffle(data)
        train_num = int(len(data) * 0.75)
        val_num = train_num + int(len(data)*0.15)
        
        train_set = data[:train_num]
        test_set = data[val_num:]
        validation_set = data[train_num:val_num]
        '''
        
        data = []
        for line in f.readlines():
            data.append(line)
        shuffle(data)
        
        train_num = int(len(data) * 0.8)
        # train_num = int(len(data) * 0.75)
        # val_num = train_num + int(len(data)*0.15)
        
        # train_set = data[:train_num]
        # test_set = data[val_num:]
        # validation_set = data[train_num:val_num]
        
        train_set = data[:train_num]
        validation_set = data[train_num:]
        
        for i, each in enumerate(train_set):
            spec = json.loads(each)
            train_path.append(spec['key'])
            train_duration.append(float(spec['duration']))
            train_text.append(spec['text'])
            if i == len(train_set) - 2:
                break
        
        # for i, each in enumerate(test_set):
        #    spec = json.loads(each)
        #    test_path.append(spec['key'])
        #    test_duration.append(float(spec['duration']))
        #    test_text.append(spec['text'])
        #    if i == len(train_set) - 2:
        #        break
        
        for i, each in enumerate(validation_set):
            spec = json.loads(each)
            validation_path.append(spec['key'])
            validation_duration.append(float(spec['duration']))
            validation_text.append(spec['text'])
            if i == len(train_set) - 2:
                break
            
    # return train_path, train_duration, train_text, test_path, test_duration, test_text, validation_path, validation_duration, validation_text
    return train_path, train_duration, train_text, validation_path, validation_duration, validation_text

        
def sort_data(input_path, input_duration, input_text):
    index = np.argsort(input_duration).tolist()
    output_path = [input_path[i] for i in index]
    output_duration = [input_duration[i] for i in index]
    output_text = [input_text[i] for i in index]
    print('Data Sorted!')
    
    return output_path, output_duration, output_text


# Featurizing:
def get_mfcc(audio_path, n_mfcc=13):
    time_series, sampling_rate = librosa.load(audio_path, sr=None)
    mfcc = np.transpose(librosa.feature.mfcc(time_series, sr=sampling_rate, n_mfcc=n_mfcc))
    
    return mfcc


def fit_train(train_path, train_duration, train_text, k_samples=100, seed=123):
    rng = random.Random(seed)
    k_samples = min(k_samples, len(train_path))
    samples = rng.sample(train_path, k_samples)
    feats = [get_mfcc(s) for s in samples]
    feats = np.vstack(feats)
    feats_mean = np.mean(feats, axis=0)
    feats_std = np.std(feats, axis=0)
    
    return feats, feats_mean, feats_std
    
    
def normalize(mfcc_feature, feats_mean, feats_std, eps=1e-14):
    return (mfcc_feature - feats_mean) / (feats_std + eps)


# Ctc-related:
def ctc_lambda_func(args):
    y_pred, labels, input_length, label_length = args
    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)


def cnn_output_length(input_length, filter_size, border_mode, stride, dilation=1):
    """ Compute the length of the output sequence after 1D convolution along
        time. Note that this function is in line with the function used in
        Convolution1D class from Keras.
    Params:
        input_length (int): Length of the input sequence.
        filter_size (int): Width of the convolution kernel.
        border_mode (str): Only support `same` or `valid`.
        stride (int): Stride size used in 1D convolution.
        dilation (int)
    """
    if input_length is None:
        return None
    assert border_mode in {'same', 'valid', 'causal'}
    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)
    if border_mode == 'same':
        output_length = input_length
    elif border_mode == 'valid':
        output_length = input_length - dilated_filter_size + 1
    elif border_mode == 'causal':
        output_length = input_length + dilated_filter_size - 1
    return (output_length + stride - 1) // stride


def add_ctc_loss(input_model):
    the_labels = Input(name='the_labels', shape=(None,), dtype='float32')
    input_lengths = Input(name='input_length', shape=(1,), dtype='int64')
    label_lengths = Input(name='label_length', shape=(1,), dtype='int64')
    output_lengths = Lambda(input_model.output_length)(input_lengths)
    
    # CTC loss is implemented in a lambda layer
    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(
        [input_model.output, the_labels, output_lengths, label_lengths])
    
    model = Model(
        inputs=[input_model.input, the_labels, input_lengths, label_lengths], 
        outputs=loss_out)
    
    return model


# Text mapping:
def text_to_int_map(text_sequence):
    character_map = {'\'':0, '<SPACE>': 1, 'a':2, 'b':3, 'c':4, 'd':5, 'e':6, 'f':7, 'g':8, 'h':9, 'i':10, 
                    'j':11, 'k':12, 'l':13, 'm':14, 'n':15, 'o':16, 'p':17, 'q':18, 'r':19, 's':20, 't':21,
                    'u':22, 'v':23, 'w':24, 'x':25, 'y':26, 'z':27}
    output_sequence = []
    for letter in text_sequence:
        if letter == ' ':
            output_sequence.append(character_map['<SPACE>'])
        else:
            output_sequence.append(character_map[letter])
    
    return output_sequence


def int_sequence_to_text(int_sequence):
    """ Convert an integer sequence to text """
    int_map = {1: "'", 2: ' ', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h',
               11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 
               20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'x', 27: 'y', 28: 'z'}
    text = []
    for c in int_sequence:
        ch = int_map[c]
        text.append(ch)
    return text


def get_batch(path, text, feats_mean, feats_std, current_index=0, minibatch_size=20, mfcc_dim=13):
    # TODO: Implement
    features = [normalize(get_mfcc(a), feats_mean, feats_std) for a in path[current_index:current_index+minibatch_size]]
    
    # calculate necessary sizes
    max_length = max([features[i].shape[0] for i in range(0, minibatch_size)])
    max_string_length = max([len(text[current_index+i]) for i in range(0, minibatch_size)])
    
    X_data = np.zeros([minibatch_size, max_length, mfcc_dim])
    labels = np.ones([minibatch_size, max_string_length]) * 28
    input_length = np.zeros([minibatch_size, 1])
    label_length= np.zeros([minibatch_size, 1])
    
    for i in range(minibatch_size):
        feat = features[i]
        input_length[i] = feat.shape[0]
        X_data[i, :feat.shape[0], :] = feat
    
        # calculate labels & label_length
        label = np.array(text_to_int_map(text[current_index+i])) 
        labels[i, :len(label)] = label
        label_length[i] = len(label)
    
    
    # return the arrays
    outputs = {'ctc': np.zeros([minibatch_size])}
    inputs = {'the_input': X_data, 
             'the_labels': labels, 
             'input_length': input_length, 
             'label_length': label_length }
    return (inputs, outputs)


def next_batch(path, duration, text, feats_mean, feats_std, current_index=0, minibatch_size=20):
    current_index = current_index
    random.seed(123)
    while True:
        ret = get_batch(path, text, feats_mean, feats_std, current_index=current_index, minibatch_size=20)
        current_index += minibatch_size
        if current_index >= len(text) - minibatch_size:
            current_index = 0
            random.Random(123).shuffle(path)
            random.Random(123).shuffle(duration)
            random.Random(123).shuffle(text)
        yield ret


# Model operations:
def train_model(
	input_model, 
	pickle_path,
	save_model_path,
	train_path,
	train_duration,
	train_text,
	validation_path,
	validation_duration,
	validation_text,
	feats_mean,
	feats_std,
	minibatch_size=20,
	mfcc_dim=13,
	learning_rate=0.02,
	epochs=20,
	verbose=1):
	
	optimizer=SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)
    # Calculating steps per epoch:
    stepPerEpoch = len(train_path)//minibatch_size
    print('Epoch step: {}'.format(stepPerEpoch))
    
    # Calculating validation steps
    stepPerVal = len(validation_path)//minibatch_size
    print('Validation step: {}'.format(stepPerVal))
    
    # add CTC loss to the NN specified in input_to_softmax
    model = add_ctc_loss(input_model)
    
    # CTC loss is implemented elsewhere, so use a dummy lambda function for the loss
    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)

    # make results/ directory, if necessary
    if not os.path.exists('results'):
        os.makedirs('results')

    # add checkpointer
    checkpointer = ModelCheckpoint(filepath='results/'+save_model_path, verbose=0)
    
    # train the model
    hist = model.fit_generator(generator=next_batch(train_path, train_duration, train_text, feats_mean=feats_mean, feats_std=feats_std), 
                               steps_per_epoch=stepPerEpoch,
                               epochs=epochs, 
                               validation_data=next_batch(validation_path, validation_duration, validation_text, feats_mean=feats_mean, feats_std=feats_std), 
                               validation_steps=stepPerVal,
                               callbacks=[checkpointer], verbose=verbose)

    # save model loss
    with open('results/'+pickle_path, 'wb') as f:
        pickle.dump(hist.history, f)


def get_predictions(index, path, text, input_model, model_path, feats_mean, feats_std):
    transcription = text[index]
    audio_path = path[index]
    data_point = normalize(get_mfcc(audio_path), feats_mean, feats_std)

    input_model.load_weights(model_path)
    prediction = input_model.predict(np.expand_dims(data_point, axis=0))
    output_length = [input_model.output_length(data_point.shape[0])]
    print(data_point.shape)
    pred_ints = (K.eval(K.ctc_decode(prediction, output_length)[0][0])+1).flatten().tolist()

    print('-'*80)
    Audio(audio_path)
    print('True transcription:\n' + '\n' + transcription)
    print('-'*80)
    print('Predicted transcription:\n' + '\n' + ''.join(int_sequence_to_text(pred_ints)))
    print('-'*80)
    