"""
Use this script to create JSON-Line description files that can be used to
train deep-speech models through this library.
This works with data directories that are organized like LibriSpeech:
data_directory/group/speaker/[file_id1.wav, file_id2.wav, ...,
                              speaker.trans.txt]

Where speaker.trans.txt has in each line, file_id transcription

NOTE: this file is from the https://github.com/baidu-research/ba-dls-deepspeech repository
"""

from __future__ import absolute_import, division, print_function

import json
import os
import librosa
import numpy as np
from random import shuffle


def create_transcript_from_folder(data_directory, group, speaker, output_file):
    labels = []
    durations = []
    keys = []
    dat_di = data_directory + '/' + group + '/' + speaker
    labels_file = os.path.join(dat_di,
                                '{}-{}.trans.txt'
                                .format(group, speaker))
    for line in open(labels_file):
        split = line.strip().split()
        file_id = split[0]
        label = ' '.join(split[1:]).lower()
        audio_file = os.path.join(data_directory, group, speaker,
                                  file_id) + '.flac'
        timeSeries, samplingRate = librosa.load(audio_file)
        duration = librosa.get_duration(timeSeries, samplingRate)
        keys.append(audio_file)
        durations.append(duration)
        labels.append(label)
    with open(output_file, 'w') as out_file:
        for i in range(len(keys)):
            line = json.dumps({'key': keys[i], 'duration': durations[i],
                              'text': labels[i]})
            out_file.write(line + '\n')
    print('Done creating {}'.format(output_file))


def create_transcript(data_directory, groups, output_file):
    labels = []
    durations = []
    keys = []
    for group in groups:
        speaker_path = os.path.join(data_directory, group)
        for speaker in os.listdir(speaker_path):
            transcript = os.path.join(speaker_path, speaker, '{}-{}.trans.txt'.format(group, speaker))
            for line in open(transcript):
                split = line.strip().split()
                file_id = split[0]
                label = ' '.join(split[1:]).lower()
                audio_file = os.path.join(data_directory, group, speaker,
                                          file_id) + '.flac'
                timeSeries, samplingRate = librosa.load(audio_file)
                duration = librosa.get_duration(timeSeries, samplingRate)
                keys.append(audio_file)
                durations.append(duration)
                labels.append(label)
            with open(output_file, 'w') as out_file:
                for i in range(len(keys)):
                    line = json.dumps({'key': keys[i], 'duration': durations[i],
                                      'text': labels[i]})
                    out_file.write(line + '\n')
    print('Done creating {}'.format(output_file))


def train_test_val_split(desc_file):
    """ 
    Reads data from a JSON-like file (generated by create_desc_json.py) and returns train_path, train_duration, train_text, test_path, test_duration, test_text, validation_path, validation_duration, validation_text
    """
    train_path, test_path, validation_path = [], [], []
    train_duration, test_duration, validation_duration = [], [], []
    train_text, test_text, validation_text = [], [], []
    
    with open(desc_file) as f:
        '''
        data = f.read().split('\n')
        shuffle(data)
        train_num = int(len(data) * 0.75)
        val_num = train_num + int(len(data)*0.15)
        
        train_set = data[:train_num]
        test_set = data[val_num:]
        validation_set = data[train_num:val_num]
        '''
        
        data = []
        for line in f.readlines():
            data.append(line)
        shuffle(data)
        
        train_num = int(len(data) * 0.75)
        val_num = train_num + int(len(data)*0.15)
        
        train_set = data[:train_num]
        test_set = data[val_num:]
        validation_set = data[train_num:val_num]
        
        for i, each in enumerate(train_set):
            spec = json.loads(each)
            train_path.append(spec['key'])
            train_duration.append(float(spec['duration']))
            train_text.append(spec['text'])
            if i == len(train_set) - 2:
                break
        
        for i, each in enumerate(test_set):
            spec = json.loads(each)
            test_path.append(spec['key'])
            test_duration.append(float(spec['duration']))
            test_text.append(spec['text'])
            if i == len(train_set) - 2:
                break
        
        for i, each in enumerate(validation_set):
            spec = json.loads(each)
            validation_path.append(spec['key'])
            validation_duration.append(float(spec['duration']))
            validation_text.append(spec['text'])
            if i == len(train_set) - 2:
                break
            
    return train_path, train_duration, train_text, test_path, test_duration, test_text, validation_path, validation_duration, validation_text
        
def sort_data(input_path, input_duration, input_text):
    index = np.argsort(input_duration).tolist()
    output_path = [input_path[i] for i in index]
    output_duration = [input_duration[i] for i in index]
    output_text = [input_text[i] for i in index]
    print('Data Sorted!')
    
    return output_path, output_duration, output_text

def get_mfcc(audio_path, n_mfcc=13):
    time_series, sampling_rate = librosa.load(audio_path, sr=None)
    mfcc = np.transpose(librosa.feature.mfcc(time_series, sr=sampling_rate, n_mfcc=n_mfcc))
    
    return mfcc

def fit_train(train_path, train_duration, train_text, k_samples=100, seed=123):
    rng = random.Random(seed)
    k_samples = min(k_samples, len(self.train_audio_paths))
    samples = rng.sample(train_path, k_samples)
    feats = [get_mfcc(s) for s in samples]
    feats = np.vstack(feats)
    feats_mean = np.mean(feats, axis=0)
    feats_std = np.std(feats, axis=0)
    
    return feats, feats_mean, feats_std
    
def normalize(mfcc_feature, feats_mean, feats_std, eps=1e-14):
    return (feature - feats_mean) / (feats_std + eps)

def train_model():
    # TODO: Implement
    pass